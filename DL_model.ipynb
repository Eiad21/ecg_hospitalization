{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b77fbb-5ee4-453d-af49-e5ba9b3138bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Iskndranii\\anaconda3\\envs\\e_hc_h\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, LSTM, BatchNormalization, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76b7297a-7ce1-4c0f-9b33-0d967d1c1f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "csv_path = 'D:\\Omar\\Friends\\European_HealthCare_Hackathon\\ecg_hospitalization\\data\\processed\\meta\\data_pairs.csv'  # Replace with the path to your CSV file\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49720c5e-5e18-4541-827a-8aca0f9ad157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists to store data\n",
    "data_list = []\n",
    "labels_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad193954-03e9-4f38-ad96-22d1fbefb660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23292it [08:30, 45.61it/s] \n"
     ]
    }
   ],
   "source": [
    "# Iterate through rows and load .npy files\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    file_path = row['np_file_path']\n",
    "    label = row['label']\n",
    "\n",
    "    # Load the .npy file\n",
    "    loaded_data = np.load(file_path)\n",
    "\n",
    "    # Append the loaded data and label to the lists\n",
    "    data_list.append(loaded_data)\n",
    "    labels_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6931334-0e1a-4d17-b336-127cef700706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to NumPy arrays\n",
    "data_array = np.array(data_list)\n",
    "labels_array = np.array(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d59654f0-9e21-4ab4-ab25-8f509c116af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Array Shape: (23292, 8, 5000)\n",
      "Labels Array Shape: (23292,)\n"
     ]
    }
   ],
   "source": [
    "# Print shapes for verification\n",
    "print(\"Data Array Shape:\", data_array.shape)\n",
    "print(\"Labels Array Shape:\", labels_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ee262a8-1471-4da2-963f-5630b88546d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_with_nans = [370, 899, 4733, 4936, 5404, 8354, 9146, 9560, 10268, 10879, 11915, 12946, 13441, 14674, 15413, 15702, 16190, 22258, 23204]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "632240cd-8be7-48e3-9f38-295ac0cfa7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = data_array[np.logical_not(np.isin(np.arange(len(data_array)), indices_with_nans))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aabbc16-1f55-4835-8f80-4aaa0a084f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_array = labels_array[np.logical_not(np.isin(np.arange(len(labels_array)), indices_with_nans))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8ad672d-b27a-4f84-a439-e0c92669ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_patients, num_leads, num_time_points = data_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d266989-79fc-4c09-8c96-a5464b6f4a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_data = data_array\n",
    "labels = labels_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6957726b-07f9-4fa4-96cc-7a21d105aaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98a47445-6daa-4da1-b23e-2c582ff22d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random indices for shuffling\n",
    "indices = np.arange(len(labels))\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b4f3f17-1c46-4dca-b685-fd64300a1d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle ecg_data and labels using the generated indices\n",
    "ecg_data_shuffled = ecg_data[indices]\n",
    "labels_shuffled = labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fe7c61b-9928-4590-92fd-2107c7155dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be moved up\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy import signal\n",
    "from scipy.fft import fft\n",
    "from biosppy.signals import ecg  # Biosppy is a library for biosignal processing\n",
    "import pywt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "501838d0-fc04-4157-9daf-e194a8cf0c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the classical models\n",
    "\n",
    "# Function to extract features from each lead\n",
    "def extract_features(lead):\n",
    "    # 1. Statistical Features\n",
    "    mean_value = np.mean(lead)\n",
    "    median_value = np.median(lead)\n",
    "    std_dev_value = np.std(lead)\n",
    "    skewness_value = skew(lead)\n",
    "    kurtosis_value = kurtosis(lead)\n",
    "\n",
    "    # 2. Time-Domain Features\n",
    "    # You might need to preprocess the data to find R-peaks for RR interval calculations\n",
    "    # Example using biosppy\n",
    "    rpeaks = ecg.ecg(lead, sampling_rate=500, show=False)['rpeaks']\n",
    "    rr_interval = np.diff(rpeaks)\n",
    "\n",
    "    # Calculate features from RR intervals\n",
    "    rr_mean = np.mean(rr_interval)  \n",
    "    heart_rate = 60 / rr_mean\n",
    "\n",
    "    # 3. Frequency-Domain Features\n",
    "    #power_spectral_density (psd) shape is (num_time_points // 2 + 1)\n",
    "    f, psd = signal.welch(lead, fs=500)\n",
    "    dominant_frequency = f[np.argmax(psd)]\n",
    "    spectral_entropy = -np.sum(psd * np.log2(psd + 1e-10))\n",
    "\n",
    "    # Combine all features into a single array\n",
    "    extracted_features = np.array([\n",
    "        mean_value, median_value, std_dev_value, skewness_value, kurtosis_value,\n",
    "        rr_mean, heart_rate, dominant_frequency, spectral_entropy\n",
    "    ])\n",
    "\n",
    "    return extracted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea6e3322-b7c3-4620-b94e-200be41bb2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = len(extract_features(ecg_data[0, 0, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7845730b-b1a2-4cf4-befa-49c7b748e836",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_features = np.zeros((num_patients, num_features * num_leads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f34b2d-a7dd-42e8-a5c3-b9082e5d9f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|██████████████████████████████████████████████████████▊                     | 16782/23273 [19:37<07:50, 13.79it/s]"
     ]
    }
   ],
   "source": [
    "# Apply feature extraction for each lead and concatenate features for each patient\n",
    "for patient_index in tqdm(range(num_patients)):\n",
    "    patient_lead_features = np.zeros((num_leads, num_features))\n",
    "    \n",
    "    for lead_index in range(num_leads):\n",
    "        patient_lead_features[lead_index, :] = extract_features(ecg_data[patient_index, lead_index, :])\n",
    "    \n",
    "    # Concatenate features for the current patient\n",
    "    patient_features[patient_index, :] = patient_lead_features.flatten()\n",
    "\n",
    "\n",
    "############## consider try - catch #######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d6947be9-e125-4b8b-8a9d-79fd5b3d77f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23280, 72)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b1c48222-3c2d-4be9-922d-506dcd08d25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23280, 8, 5000)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecg_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bffeaafa-7696-4093-bea5-fce18cd7d3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23280,)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9add3c6a-3eb2-4dd2-af56-e6d2781e3d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_classical = patient_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "33d79698-365f-403a-8fce-d51ec8c07206",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_deep = ecg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fe74c204-55dc-4d83-8beb-47a79059ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train_classical, X_test_classical, y_train_classical, y_test_classical = train_test_split(X_classical, labels, test_size=0.2, random_state=42)\n",
    "X_train_deep, X_test_deep, y_train_deep, y_test_deep = train_test_split(ecg_data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0813d9f8-c5c5-4bbc-a17e-1e143a350ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_train_classical)):\n",
    "    if(y_train_classical[i] != y_train_deep[i]):\n",
    "        Print(\"WRONG SPLITTING!!!!!!!!!!!  TRAIN\")\n",
    "\n",
    "for i in range(len(y_test_classical)):\n",
    "    if(y_test_classical[i] != y_test_deep[i]):\n",
    "        Print(\"WRONG SPLITTING!!!!!!!!!!!   TESTTTTTTTTTT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b236ad1e-100f-4e25-83af-41810be135c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train_classical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "15fed78a-3328-449f-abd3-d9ce5ddc7de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train_deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1e7de554-4913-4736-85c5-cf992135316b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4791060c-391d-497e-aa2b-10ef90fd7e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 2, 0])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3900bdfd-8976-428a-8a17-65dbfb4468c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ DEEP ###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "95a4e3a6-6b4a-4cbd-bde8-3af866746d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the CNN combined with RNN model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=(8, 5000)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(128, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))  # Assuming 3 output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bb5292c9-97ef-4d00-bcc7-8fbc9d27200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dcdfc1d7-2d47-4e55-8c79-fc50101f860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c75aed89-b025-4010-9cfa-633d476b46e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement early stopping based on validation loss\n",
    "#early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "569c7b9c-a696-4b8d-bd53-b0e70b26f342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ce6a5318-9a2c-4d67-86c3-d3a591a8e69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to categorical format\n",
    "le = LabelEncoder()\n",
    "y_train_deep = to_categorical(le.fit_transform(y_train_deep))\n",
    "y_test_deep = to_categorical(le.fit_transform(y_test_deep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d30e8182-de60-4419-b554-a0f033b05b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class proportions\n",
    "p1 = 0.61\n",
    "p2 = 0.29\n",
    "p3 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "44363875-ddf9-4a0f-95cb-21d61aa51291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weights\n",
    "w1 = 1 / p1\n",
    "w2 = 1 / p2\n",
    "w3 = 1 / p3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "37ac069c-f3e1-45be-9af0-799e464df12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize weights\n",
    "sum_weights = w1 + w2 + w3\n",
    "normalized_w1 = w1 / sum_weights\n",
    "normalized_w2 = w2 / sum_weights\n",
    "normalized_w3 = w3 / sum_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "715ea94b-e565-4a17-b94d-a8339afc6322",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 5.55 GiB for an array with shape (18624, 8, 5000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train_deep, y_train_deep, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, validation_data\u001b[38;5;241m=\u001b[39m(X_test_deep, y_test_deep), class_weight\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m0\u001b[39m:normalized_w1, \u001b[38;5;241m1\u001b[39m:normalized_w2, \u001b[38;5;241m2\u001b[39m:normalized_w3})\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\e_hc_h\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\e_hc_h\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:91\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to an `EagerTensor`.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03mNote that this function could return cached copies of created constants for\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m  TypeError: if `dtype` is not compatible with the type of t.\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m     88\u001b[0m   \u001b[38;5;66;03m# Make a copy explicitly because the EagerTensor might share the underlying\u001b[39;00m\n\u001b[0;32m     89\u001b[0m   \u001b[38;5;66;03m# memory with the input array. Without this copy, users will be able to\u001b[39;00m\n\u001b[0;32m     90\u001b[0m   \u001b[38;5;66;03m# modify the EagerTensor after its creation by changing the input array.\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m   value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, ops\u001b[38;5;241m.\u001b[39mEagerTensor):\n\u001b[0;32m     93\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 5.55 GiB for an array with shape (18624, 8, 5000) and data type float64"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train_deep, y_train_deep, epochs=50, batch_size=batch_size, validation_data=(X_test_deep, y_test_deep), class_weight={0:normalized_w1, 1:normalized_w2, 2:normalized_w3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cf858c7b-916e-46cf-bc31-c23e6ac7a73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 1s 4ms/step - loss: 2.8677 - accuracy: 0.4640\n",
      "Test Loss: 2.8677122592926025\n",
      "Test Accuracy: 0.4640480875968933\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_deep, y_test_deep)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56a1bf9-0367-495c-859e-cadfd33e388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_deep = model.predict(X_test_deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc811a9-3566-424c-b940-dc5d2b9997ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test_deep, y_pred_deep)\n",
    "report = classification_report(y_test_deep, y_pred_deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0634d7-b122-4c66-9ed6-073cc8185fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7735bde2-70e2-4be7-9183-0778cc62573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ CLASSICAL #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d130814f-8c39-4fc6-81ab-bdf29a2f1312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_classical = scaler.fit_transform(X_train_classical)\n",
    "X_test_classical = scaler.transform(X_test_classical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acf7f0e-cd48-436d-a062-13536514e530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(class_weight = {0 : 0.05, 1 : 0.1, 2 : 0.85}, n_estimators=150, random_state=42)\n",
    "rf_classifier.fit(X_train_classical, y_train_classical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f4d87d-db43-4dd2-8f86-2ff745c72285",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_classical = rf_classifier.predict(X_test_classical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026273c9-713c-4872-a9fd-7d6eeafdcb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test_classical, y_pred_classical)\n",
    "report = classification_report(y_test_classical, y_pred_classical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db1b7fb-22d8-4b4e-b20b-f235824d07e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8df570-7c04-42fe-8bbd-71b644848cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### ENSEMBLE ###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a50931-1e7d-4dd2-a6fb-f373fd5bb4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Combine predictions using stacking\n",
    "stacked_features_train = np.column_stack((y_pred_deep, y_pred_classical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d7517e-db00-4d1a-af9c-d09352bed54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MOVE above later\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affcc61f-240d-4715-bd9d-00a126fde171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Train a logistic regression meta-model\n",
    "meta_model = LogisticRegression()\n",
    "meta_model.fit(stacked_features_train, y_test_deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f79953e-718f-49d4-a7ab-a1b9563ef8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cross_val_predict for out-of-fold predictions\n",
    "# y_pred_deep ready\n",
    "# y_pred_classical ready\n",
    "stacked_features_test = np.column_stack((y_pred_deep, y_pred_classical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a29e37-5f06-493d-af8d-896a3b35a564",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ensemble = meta_model.predict(stacked_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373eeba1-6809-4537-9824-41924568ecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Evaluate the ensemble\n",
    "ensemble_accuracy = accuracy_score(y_test_deep, y_pred_ensemble)\n",
    "print(f'Ensemble Accuracy: {ensemble_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9abc7d-13a5-4b4f-954c-2c157a9be93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model_cnn = keras.Sequential([\n",
    "    layers.Conv1D(64, 3, activation='relu', input_shape=(stacked_features_train.shape[1], 1)),\n",
    "    layers.MaxPooling1D(2),\n",
    "    layers.Conv1D(128, 3, activation='relu'),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')  # Output layer with 3 classes\n",
    "])\n",
    "\n",
    "meta_model_cnn.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "meta_model_cnn.fit(stacked_features_train, y_test_deep, epochs=60, batch_size=32, validation_split=0.15)\n",
    "\n",
    "\n",
    "\n",
    "y_pred_ensemble_cnn = meta_model_cnn.predict(stacked_features_test)\n",
    "\n",
    "# Step 8: Evaluate the ensemble\n",
    "ensemble_accuracy = accuracy_score(y_test_deep, np.argmax(y_pred_ensemble_cnn, axis=1))\n",
    "print(f'Ensemble Accuracy: {ensemble_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9293b618-fd12-48cc-bcf9-3c609c063555",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
